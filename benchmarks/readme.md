## The details of applying FACMAC, MASAC and MAMBPO to the MuRES problem are as follows:

- FACMAC: We train the FACMAC baseline for the MuRES problem based on the publicly available FACMAC source code(https://github.com/oxwhirl/facmac). For the adapted FACMAC baseline, we employ the original model architecture, which can be interpreted as increasing the number of Critic networks to $N$ on the basic structure of MADDPG, and then utilizing the mixer of QMIX to fuse the $Q$ values generated by Critic networks. These $N$ Actor and N Critic networks all use linear connection layers. The input of the Actor network is the observation of a single agent (the same as state type in CE-PG), the output is the logits of all possible actions, and the mask is applied according to the structure of the searching environment in order to sample the executable actions. The input of the Critic network is the observations of all the agents and the output is the factorized $Q$-value.
- MASAC: We implement the MASAC baseline for the MuRES problem based on the publicly released SAC code(https://github.com/toshikwa/sac-discrete.pytorch), combined with the VDN mixer. The adapted MACSAC baseline consists of $N$ Actor networks and $N$ Critic networks, all of which have linear connection layers. The input of Actor network is the observation of a single agent and the output is the logits of all actions, while the input of Critic network is the state of the single agent and the two Q-values generated by Critic network are mixed by the mixer.
- MAMBPO: We develop the MAMBPO baseline for the MuRES problem based on the publicly released MAMBPO code (https://github.com/danielwillemsen/mambpo). For the adapted MAMBPO baseline, we employed the original model architecture, which consists of $N$ Actor networks, $N$ Critic networks and $N$ predict networks. These networks are composed of linear connectivity layers. Actor-Critic takes the SAC framework, where the inputs to the Actor network are observations of individual agent and the outputs are logits of all actions. Critic network's input is the state of a single agent and output two Q-values which are mixed by VDN-mixer. The predict network outputs the mean and variance of the future state under the current policy and the mean and variance of the current reward after receiving the observations of individual agent.
